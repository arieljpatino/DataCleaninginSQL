Objective: The goal of this project is to clean and preprocess a large Nashville Housing dataset to ensure data quality and consistency, making it suitable for further analysis and reporting.

Description: This project involves the use of SQL to clean a large dataset containing various types of data anomalies. The dataset includes missing values, duplicate records, inconsistent data formats, etc. The key tasks in this project are:

    Removing Irrelevant Data: Identify and delete columns or rows that are not necessary for the analysis.
    Handling Missing Values: Use SQL functions to fill in missing values or remove rows with significant missing data.
    Removing Duplicates: Identify and remove duplicate records to ensure each entry is unique.
    Fixing Structural Errors: Correct inconsistencies in data formats, such as date formats and string capitalization.
    Type Conversion: Convert data types to ensure consistency across the dataset (e.g., converting strings to dates).
    Standardizing Data: Normalize data to ensure uniformity, such as standardizing units of measurement.
    Validating Data: Perform checks to ensure the cleaned data meets the required quality standards.

Tools and Technologies:
    SQL (Structured Query Language)

Expected Outcome: A cleaned and well-structured dataset that is ready for analysis, leading to more accurate and reliable insights.

Files: 
    RAW Nashville Housing Data.xlsx: Completely raw and untouched dataset concerning Nashville Housing records (with over 56,000 records) to be examined and thoroughly cleaned after being imported into SQL SSMS.
    DataCleaningProject.SQL: Subsequent SQL code used to clean the large dataset to meet expectations of outcome. 
    Clean Nashville Housing Data.xlsx: Cleaned dataset after being put through SQL code, and exported back to an Excel file.
    
